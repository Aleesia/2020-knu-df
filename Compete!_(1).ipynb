{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Compete!_(1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aleesia/2020-knu-df/blob/a.melekestseva/Compete!_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYvrI1FptDez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qforgHvvd_N",
        "colab_type": "code",
        "outputId": "89ce493a-9b6a-4132-b0b3-d5b9fa9d9b3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdVL_lIKwWpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q  \"/content/gdrive/My Drive/deep-fake-detection-knu-2020.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7G0Fctpb__0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "fake_images = os.listdir('train/train/1_fake')\n",
        "train_fakes = fake_images[:51000]\n",
        "val_fakes = fake_images[51000:]\n",
        "real_images = os.listdir('train/train/0_real')\n",
        "train_reals = real_images[:51000]\n",
        "val_reals = real_images[51000:]\n",
        "\n",
        "def copyfile(filename, fromdir, todir):\n",
        "    fromfullpath = os.path.join(fromdir, filename)\n",
        "    tofullpath = os.path.join(todir, filename)\n",
        "    fromfile = open(fromfullpath, \"rb\")\n",
        "    tofile = open(tofullpath, \"wb\")\n",
        "    cnt = fromfile.read()\n",
        "    tofile.write(cnt)\n",
        "    fromfile.close()\n",
        "    tofile.close()\n",
        "\n",
        "os.mkdir('my_data')\n",
        "os.mkdir('my_data/train')\n",
        "os.mkdir('my_data/val')\n",
        "os.mkdir('my_data/train/1_fake')\n",
        "os.mkdir('my_data/val/1_fake')\n",
        "os.mkdir('my_data/train/0_real')\n",
        "os.mkdir('my_data/val/0_real')\n",
        "\n",
        "for image in train_fakes:\n",
        "    copyfile(image, 'train/train/1_fake', 'my_data/train/1_fake')\n",
        "    os.remove('train/train/1_fake/'+image)\n",
        "for image in val_fakes:\n",
        "    copyfile(image, 'train/train/1_fake', 'my_data/val/1_fake')\n",
        "    os.remove('train/train/1_fake/'+image)\n",
        "for image in train_reals:\n",
        "    copyfile(image, 'train/train/0_real', 'my_data/train/0_real')\n",
        "    os.remove('train/train/0_real/'+image)\n",
        "for image in val_reals:\n",
        "    copyfile(image, 'train/train/0_real', 'my_data/val/0_real')\n",
        "    os.remove('train/train/0_real/'+image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfkJ5j4JlB-3",
        "colab_type": "code",
        "outputId": "c53aa949-968d-4d18-925d-dbd5f98fe9ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(len(os.listdir('my_data/val/1_fake')))\n",
        "print(len(os.listdir('my_data/val/0_real')))\n",
        "print(len(os.listdir('my_data/train/1_fake')))\n",
        "print(len(os.listdir('my_data/train/0_real')))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9000\n",
            "9000\n",
            "51000\n",
            "51000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH-l4fgXtUWr",
        "colab_type": "code",
        "outputId": "032980b9-a12c-415b-f783-02740d9bcadd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "train_data_path = 'my_data/train/'\n",
        "val_data_path = 'my_data/val/'\n",
        "VAL_SIZE = 0.15 # percentage of data for validation\n",
        "\n",
        "transform = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
        "         transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform)\n",
        "val_dataset = torchvision.datasets.ImageFolder(root=val_data_path, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = BATCH_SIZE,\n",
        "        shuffle=True, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = BATCH_SIZE, num_workers=2)\n",
        "\n",
        "print(f\"Length train: {len(train_loader.dataset)}\")\n",
        "print(f\"Length valid: {len(val_loader.dataset)}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length train: 102000\n",
            "Length valid: 18000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-IDRoJQdZLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_path = \"/content/test\"\n",
        "test_dataset = torchvision.datasets.ImageFolder(root=test_data_path, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = BATCH_SIZE,\n",
        "        shuffle=False, num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-HeysJPYlMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2B4L-8aZdkS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Net().to(device)\n",
        "import math\n",
        "\n",
        "\n",
        "def log_loss(output, target):\n",
        "    if output==1:\n",
        "        return 0\n",
        "    return target * math.log(output) + (1-target) * math.log(1-output)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 0.05\n",
        "momentum = 0.9\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8Qvmiff0SU0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7ca41f28-0250-422a-ae97-3057ebe49f9d"
      },
      "source": [
        "epoch_num = 12\n",
        "for epoch in range(1, epoch_num+1):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:\n",
        "            print(f'[Epoch {epoch}: batch {i + 1}] loss: {running_loss / 200}')\n",
        "            running_loss = 0.0\n",
        "    if epoch % 4 == 0:\n",
        "        lr = lr*0.7\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb9f84c4fd0>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb9f84c4fd0>>\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb9fc4587b8>>\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb9f8106f28>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb9fc4587b8>>\n",
            "Traceback (most recent call last):\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb9f810d080>>\n",
            "Traceback (most recent call last):\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "AssertionError: can only join a child process\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb9f8106f28>>\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "AssertionError: can only join a child process\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb9f810d080>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 1: batch 200] loss: 0.6933954557776452\n",
            "[Epoch 1: batch 400] loss: 0.6893755969405174\n",
            "[Epoch 1: batch 600] loss: 0.6702169981598854\n",
            "[Epoch 1: batch 800] loss: 0.6513182288408279\n",
            "[Epoch 1: batch 1000] loss: 0.6249903136491776\n",
            "[Epoch 1: batch 1200] loss: 0.5925528402626514\n",
            "[Epoch 1: batch 1400] loss: 0.5731888145208359\n",
            "[Epoch 2: batch 200] loss: 0.5460864414274692\n",
            "[Epoch 2: batch 400] loss: 0.5425091649591923\n",
            "[Epoch 2: batch 600] loss: 0.5352892991900444\n",
            "[Epoch 2: batch 800] loss: 0.5219214032590389\n",
            "[Epoch 2: batch 1000] loss: 0.5100987572968007\n",
            "[Epoch 2: batch 1200] loss: 0.506058894842863\n",
            "[Epoch 2: batch 1400] loss: 0.506723916977644\n",
            "[Epoch 3: batch 200] loss: 0.4969533234834671\n",
            "[Epoch 3: batch 400] loss: 0.48572642982006076\n",
            "[Epoch 3: batch 600] loss: 0.4978052355349064\n",
            "[Epoch 3: batch 800] loss: 0.48603402704000476\n",
            "[Epoch 3: batch 1000] loss: 0.48464425951242446\n",
            "[Epoch 3: batch 1200] loss: 0.47675515413284303\n",
            "[Epoch 3: batch 1400] loss: 0.49244742915034295\n",
            "[Epoch 4: batch 200] loss: 0.49319674804806707\n",
            "[Epoch 4: batch 400] loss: 0.4856972301006317\n",
            "[Epoch 4: batch 600] loss: 0.4804266805946827\n",
            "[Epoch 4: batch 800] loss: 0.4824875570833683\n",
            "[Epoch 4: batch 1000] loss: 0.4906358395516872\n",
            "[Epoch 4: batch 1200] loss: 0.4828844673931599\n",
            "[Epoch 4: batch 1400] loss: 0.4791560114920139\n",
            "[Epoch 5: batch 200] loss: 0.47556438013911245\n",
            "[Epoch 5: batch 400] loss: 0.4846368533372879\n",
            "[Epoch 5: batch 600] loss: 0.4826847340166569\n",
            "[Epoch 5: batch 800] loss: 0.46667558073997495\n",
            "[Epoch 5: batch 1000] loss: 0.47983855873346326\n",
            "[Epoch 5: batch 1200] loss: 0.47458846762776374\n",
            "[Epoch 5: batch 1400] loss: 0.4827803573012352\n",
            "[Epoch 6: batch 200] loss: 0.4802937929332256\n",
            "[Epoch 6: batch 400] loss: 0.4890490025281906\n",
            "[Epoch 6: batch 600] loss: 0.47813948929309846\n",
            "[Epoch 6: batch 800] loss: 0.478731085807085\n",
            "[Epoch 6: batch 1000] loss: 0.4920970223844051\n",
            "[Epoch 6: batch 1200] loss: 0.48833013415336607\n",
            "[Epoch 6: batch 1400] loss: 0.4828809535503387\n",
            "[Epoch 7: batch 200] loss: 0.4902448754012585\n",
            "[Epoch 7: batch 400] loss: 0.510195999443531\n",
            "[Epoch 7: batch 600] loss: 0.4831413970887661\n",
            "[Epoch 7: batch 800] loss: 0.48475272960960863\n",
            "[Epoch 7: batch 1000] loss: 0.4887250034511089\n",
            "[Epoch 7: batch 1200] loss: 0.4891052205860615\n",
            "[Epoch 7: batch 1400] loss: 0.4738465295732021\n",
            "[Epoch 8: batch 200] loss: 0.46849643588066103\n",
            "[Epoch 8: batch 400] loss: 0.46052080556750297\n",
            "[Epoch 8: batch 600] loss: 0.4735317052900791\n",
            "[Epoch 8: batch 800] loss: 0.4889292427897453\n",
            "[Epoch 8: batch 1000] loss: 0.46874005407094954\n",
            "[Epoch 8: batch 1200] loss: 0.4740046617388725\n",
            "[Epoch 8: batch 1400] loss: 0.5015701669454574\n",
            "[Epoch 9: batch 200] loss: 0.45142810776829717\n",
            "[Epoch 9: batch 400] loss: 0.4508116239309311\n",
            "[Epoch 9: batch 600] loss: 0.4284379266947508\n",
            "[Epoch 9: batch 800] loss: 0.4362841664254665\n",
            "[Epoch 9: batch 1000] loss: 0.4189960809051991\n",
            "[Epoch 9: batch 1200] loss: 0.42499735951423645\n",
            "[Epoch 9: batch 1400] loss: 0.43336186170578\n",
            "[Epoch 10: batch 200] loss: 0.4141149708628655\n",
            "[Epoch 10: batch 400] loss: 0.4141048961132765\n",
            "[Epoch 10: batch 600] loss: 0.4211817544698715\n",
            "[Epoch 10: batch 800] loss: 0.4219164353609085\n",
            "[Epoch 10: batch 1000] loss: 0.40575580462813376\n",
            "[Epoch 10: batch 1200] loss: 0.40987075336277484\n",
            "[Epoch 10: batch 1400] loss: 0.41774228364229204\n",
            "[Epoch 11: batch 200] loss: 0.40880165450274947\n",
            "[Epoch 11: batch 400] loss: 0.43168005615472793\n",
            "[Epoch 11: batch 600] loss: 0.4027885214984417\n",
            "[Epoch 11: batch 800] loss: 0.3995557290315628\n",
            "[Epoch 11: batch 1000] loss: 0.4265848496556282\n",
            "[Epoch 11: batch 1200] loss: 0.4306648326665163\n",
            "[Epoch 11: batch 1400] loss: 0.42150422677397725\n",
            "[Epoch 12: batch 200] loss: 0.41261669270694257\n",
            "[Epoch 12: batch 400] loss: 0.40564763359725475\n",
            "[Epoch 12: batch 600] loss: 0.40806515611708166\n",
            "[Epoch 12: batch 800] loss: 0.41404865331947804\n",
            "[Epoch 12: batch 1000] loss: 0.399272243976593\n",
            "[Epoch 12: batch 1200] loss: 0.4208636401593685\n",
            "[Epoch 12: batch 1400] loss: 0.3929598572105169\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGhJJp0jZ05K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8f34fa7c-7907-4a92-db7a-3a5eccfca98f"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "total_correct = 0\n",
        "avg_loss = 0.0\n",
        "total_loss = 0.0\n",
        "count = 0\n",
        "for i, data in enumerate(val_loader, 0):\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    output = model(inputs)\n",
        "    avg_loss += criterion(output, labels).sum()\n",
        "    for j in range(output.shape[0]):\n",
        "        out = output[j]\n",
        "        probability = torch.softmax(out, -1)[1].cpu().item()\n",
        "        label = labels[j].cpu().item()\n",
        "        total_loss += log_loss(probability, label)\n",
        "        count += 1\n",
        "    \n",
        "    pred = output.detach().max(1)[1]\n",
        "    total_correct += pred.eq(labels.view_as(pred)).sum()\n",
        "\n",
        "total_loss  = -total_loss/count\n",
        "print(\"log loss = \", total_loss)\n",
        "\n",
        "avg_loss /= len(val_loader.dataset)\n",
        "avg_loss = avg_loss.detach().cpu().item()\n",
        "accuracy = float(total_correct) / len(val_loader.dataset)\n",
        "print(f'Test Avg. Loss: {avg_loss}, Accuracy: {accuracy}')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log loss =  0.36487561005692326\n",
            "Test Avg. Loss: 0.005715790670365095, Accuracy: 0.8449444444444445\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yopL1belu224",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "\n",
        "from PIL import Image\n",
        "dir = \"/content/test/test\"\n",
        "files = os.listdir(dir)\n",
        "results = []\n",
        "for f in files:\n",
        "    image = Image.open(dir+\"/\"+f)\n",
        "    normalized_image = transform(image)\n",
        "    normalized_image = normalized_image.unsqueeze(0)\n",
        "    normalized_image = normalized_image.to(device)\n",
        "    prediction = model(normalized_image)\n",
        "    prediction = prediction[0]\n",
        "    probabilities = torch.softmax(prediction, -1)\n",
        "    probability = probabilities[1].detach().cpu().item()\n",
        "    results.append((f, probability))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpd4U1P0GGA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "\n",
        "with open('CrossEntropyLoss_net_results.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"imagename\", \"is_fake\"])\n",
        "    for el in results:\n",
        "        writer.writerow([el[0], el[1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CysMiAztTvH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "save_name = 'NLLLoss_results.csv'\n",
        "path = F\"/content/gdrive/My Drive/{save_name}\" \n",
        "torch.save(\"/content/NLLLoss_results.csv\", path)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}